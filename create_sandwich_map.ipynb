{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build me a map, ChatGPT\n",
    "\n",
    "#### Prompt 0:\n",
    "\n",
    "Build me a sandwich map from this website:\n",
    "57 Sandwiches That Define New York City\n",
    "\n",
    "URL: https://www.nytimes.com/interactive/2024/dining/best-nyc-sandwiches.html  \n",
    "\n",
    "\n",
    "#### Prompt 1:\n",
    "\n",
    "I need you to scrape the website located at the URL below and create me a sandwich map.\n",
    "\n",
    "This is the URL:\n",
    "https://www.nytimes.com/interactive/2024/dining/best-nyc-sandwiches.html\n",
    "\n",
    "\n",
    "#### Prompt 2:\n",
    "\n",
    "I need to parse the website at the url below and extract the following items:\n",
    "- sandwich names \n",
    "- restaurant names\n",
    "- addresses\n",
    "- description\n",
    "- images\n",
    "\n",
    "This is the URL:\n",
    "https://www.nytimes.com/interactive/2024/dining/best-nyc-sandwiches.html\n",
    "\n",
    "#### ChatGPT Response:\n",
    "\n",
    "\"​I'm unable to access the specific content of the New York Times article at the provided URL.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Python code to scrape the website for me, ChatGPT\n",
    "\n",
    "#### Prompt:\n",
    "\n",
    "Write python code to parse HTML for the following items. Use beautiful soup.\n",
    "- sandwich-names: class=\"sandwich-name\" \n",
    "- restaurant-names: class=\"restaurant-name\"\n",
    "- meta-data: class=\"metadata svelte-1w102ch\"\n",
    "- description: class=\"g-text svelte-1w102ch\"\n",
    "- images: class=\"g-image svelte-z1ymna\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data from HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Fetch the webpage content\n",
    "url = \"https://www.nytimes.com/interactive/2024/dining/best-nyc-sandwiches.html\"  # Replace with the actual URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(soup.find_all(class_=\"sandwich-name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get `restaurants`, `sandwhich_names`, `meta_data`, `descriptions`\n",
    "\n",
    "- sandwich-name\n",
    "- restaurant-name\n",
    "- metadata svelte-1w102ch\n",
    "- g-text svelte-1w102ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandwich_names = []\n",
    "restaurants = []\n",
    "meta_data = []\n",
    "descriptions = []\n",
    "\n",
    "for tag in soup.find_all(class_=\"sandwich-name\"):\n",
    "    sandwich_names.append(tag.text.strip())\n",
    "\n",
    "for tag in soup.find_all(class_=\"restaurant-name\"):\n",
    "    restaurants.append(tag.text.strip())\n",
    "\n",
    "for tag in soup.find_all(class_=\"metadata svelte-1w102ch\"):\n",
    "    meta_data.append(tag.text.strip())\n",
    "\n",
    "for tag in soup.find_all(class_=\"g-text svelte-1w102ch\"):\n",
    "    descriptions.append(tag.text.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print length of every list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Length of restaurants:\", len(restaurants))\n",
    "# print(\"Length of sandwich_names:\", len(sandwich_names))\n",
    "# print(\"Length of meta_data:\", len(meta_data))\n",
    "# print(\"Length of descriptions:\", len(descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print first and last item for every list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First item of restaurants:\", restaurants[0])\n",
    "print(\"Last item of restaurants:\", restaurants[-1])\n",
    "\n",
    "print(\"First item of sandwich_names:\", sandwich_names[0])\n",
    "print(\"Last item of sandwich_names:\", sandwich_names[-1])\n",
    "\n",
    "print(\"First item of meta_data:\",meta_data[0])\n",
    "print(\"Last item of meta_data:\",meta_data[-1])\n",
    "\n",
    "print(\"First item of descriptions:\", descriptions[0])\n",
    "print(\"Last item of descriptions:\", descriptions[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images need special handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prompt:*\n",
    "\n",
    "I need to find all the images with this class:\n",
    "find_all(class_=\"g-image\")\n",
    "\n",
    "- If the image URL does not contain \"big_assets\", store it in a list.\n",
    "- if the image URL contains \"big_assets\" make sure it also ends with \"-2.png\" and store it in a list\n",
    "\n",
    "I am trying to drop images that have \"big_assests\" in the URL but end with \"-1.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the images\n",
    "images = soup.find_all(\"img\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on at the beginning of the list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_short = images[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 9 Sandwich Categories, each with an extra image\n",
    "\n",
    "- Breakfast Bangers\n",
    "- Hero Worship\n",
    "- Veg In\n",
    "- Pastrami City\n",
    "- Gotham Greats\n",
    "- ‘Let Me Get Uhh…’\n",
    "- Diner Party\n",
    "- Honorary New Yorkers\n",
    "- Extremely Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "57 + 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If URL contains 'big_assest' ensure it ends with '-2.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store valid image URLs\n",
    "valid_images = []\n",
    "\n",
    "for img in images_short:\n",
    "    img_url = img.get(\"src\")  # Get the image URL\n",
    "    if img_url:\n",
    "        if \"big_assets\" in img_url:\n",
    "            if img_url.endswith(\"-2.png\"):  # Keep only if it ends with \"-2.png\"\n",
    "                valid_images.append(img_url)\n",
    "        else:\n",
    "            valid_images.append(img_url)  # Keep all other images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First image:\", valid_images[0])\n",
    "print(\"Last image:\", valid_images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables we have created so far\n",
    "\n",
    "restaurants  \n",
    "sandwich_names  \n",
    "meta_data  \n",
    "descriptions  \n",
    "valid_images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Length of meta_data:\", len(meta_data))\n",
    "# print(\"Length of descriptions:\", len(descriptions))\n",
    "# print(\"Length of restaurants:\", len(restaurants))\n",
    "# print(\"Length of sandwich_names:\", len(sandwich_names))\n",
    "# print(\"Length of valid_images:\", len(valid_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"First element of meta_data:\", meta_data[0])\n",
    "# print(\"First element of descriptions:\", descriptions[0])\n",
    "# print(\"First element of restaurants:\", restaurants[0])\n",
    "# print(\"First element of sandwich_names:\", sandwich_names[0])\n",
    "# print(\"First element of valid_images:\", valid_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove bodega info :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://static01.nytimes.com/newsgraphics/2024-05-21-sandwich/_big_assets/breakfast-2.png'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions.pop(0)\n",
    "restaurants.pop(0)\n",
    "sandwich_names.pop(0)\n",
    "valid_images.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Length of meta_data:\", len(meta_data))\n",
    "# print(\"Length of descriptions:\", len(descriptions))\n",
    "# print(\"Length of restaurants:\", len(restaurants))\n",
    "# print(\"Length of sandwich_names:\", len(sandwich_names))\n",
    "# print(\"Length of valid_images:\", len(valid_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"First element of meta_data:\", meta_data[0])\n",
    "# print(\"First element of descriptions:\", descriptions[0])\n",
    "# print(\"First element of restaurants:\", restaurants[0])\n",
    "# print(\"First element of sandwich_names:\", sandwich_names[0])\n",
    "# print(\"First element of valid_images:\", valid_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Last element of meta_data:\", meta_data[-1])\n",
    "# print(\"Last element of descriptions:\", descriptions[-1])\n",
    "# print(\"Last element of restaurants:\", restaurants[-1])\n",
    "# print(\"Last element of sandwich_names:\", sandwich_names[-1])\n",
    "# print(\"Last element of valid_images:\", valid_images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn the lists into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'restaurant': restaurants,\n",
    "    'sandwich_name': sandwich_names,\n",
    "    'meta_data': meta_data,\n",
    "    'description': descriptions,\n",
    "    'image_url': valid_images\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is this tidy data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['sandwich_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From `meta_data`, I need to parse out `price` and `website`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['meta_data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT: I want to draw a random row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_index = random.randint(0, len(df) - 1)\n",
    "\n",
    "df['meta_data'][random_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['meta_data'][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the string by commas and get the last element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['meta_data'].str.split(',')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['meta_data'].str.split(',')[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['website'] = df['meta_data'].str.split(',').str[-1].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'candbnyc.com'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['website'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dailyprovisionsnyc.com'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['website'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shakeshack.com/location/jfk-airport-nyc'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['website'][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the string by commas and get the first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['address'] = df['meta_data'].str.split(',').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['address'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['address'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['address'][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the string by commas and get the second element, remove '$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['meta_data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'] = df['meta_data'].str.split(',').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['price'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['price'][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prompt*\n",
    "\n",
    "- in the column df['meta_data'], the price starts with a \"$\"\n",
    "- grab the integer following the \"$\"\n",
    "- and store it in a new column called 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# The pattern r'\\$(\\d+)' looks for a dollar sign ($) followed by one or more digits (\\d+)\n",
    "# .group(1): Extracts the first capturing group from the match, which corresponds to the digits following the dollar sign.\n",
    "# int(...): Converts the extracted string of digits into an integer.\n",
    "\n",
    "df['price'] = df['meta_data'].apply(lambda x: int(re.search(r'\\$(\\d+)', x).group(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df['price'].mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].value_counts().sort_index().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEOCODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prompt*  \n",
    "\n",
    "I want to geo code addresses using the google maps api. The addresses are stored in the column 'address'. I want to send the api the addresses and then get the lat and long as new columns in my df. I also want to return the status of the api call and the formatted address from the api. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: We know we have some addresses that definitley won't work -- \"Multiple Addresses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['address'].str.contains('Multiple').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get my API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key\n",
    "google_maps_api_key = os.getenv(\"GOOGLE_MAPS_GEO_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import googlemaps\n",
    "import time\n",
    "\n",
    "# Initialize Google Maps API client\n",
    "# API_KEY = \"YOUR_GOOGLE_MAPS_API_KEY\"\n",
    "gmaps = googlemaps.Client(key=google_maps_api_key)\n",
    "\n",
    "def geocode_address(address):\n",
    "    \"\"\"Geocode an address using Google Maps API and return lat, long, status, and formatted address.\"\"\"\n",
    "    try:\n",
    "        response = gmaps.geocode(address)\n",
    "        if response:\n",
    "            location = response[0]['geometry']['location']\n",
    "            formatted_address = response[0]['formatted_address']\n",
    "            return location['lat'], location['lng'], \"OK\", formatted_address\n",
    "        else:\n",
    "            return None, None, \"ZERO_RESULTS\", None\n",
    "    except Exception as e:\n",
    "        return None, None, \"ERROR\", str(e)\n",
    "\n",
    "# Apply geocoding function to each address\n",
    "df[['latitude', 'longitude', 'api_status', 'formatted_address']] = df['address'].apply(lambda x: pd.Series(geocode_address(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['sandwich_name', 'restaurant', 'address', 'formatted_address', 'latitude', 'longitude', 'api_status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['api_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split 'OK' addresses into their own dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_ok = df[df['api_status'] == 'OK'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df_ok.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the addresses look reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df_ok[['address', 'formatted_address']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split 'ZERO_RESULTS' addresses into their own dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_bad = df[df['api_status'] == 'ZERO_RESULTS'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_bad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(geo_df_bad) + len(geo_df_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the unsuccessful addresses look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_bad[[\n",
    "    'sandwich_name', \n",
    "    'restaurant',\n",
    "    'latitude',\t\n",
    "    'longitude', \n",
    "    'api_status', \n",
    "    'address',\n",
    "    'formatted_address',\n",
    "    'price'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prompt*\n",
    "\n",
    "Query google maps api by resturant name, wich is stored in the column geo_df_bad['restaurant']. On the same df, return the columns 'formatted_address' (from the api) , 'restaurant_name_api' (from the api), 'latitude', 'longitude', and 'api_status'. Many of the restaurants have multiple addresses, each address should have it's own row.\n",
    "\n",
    "NOTE: this took me into a pita pit of despair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df_bad.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import googlemaps \n",
    " \n",
    "gmaps = googlemaps.Client(key=google_maps_api_key)\n",
    "\n",
    "\n",
    "# Initialize an empty list to collect results\n",
    "geocode_results_bad = []\n",
    "\n",
    "# Loop through the DataFrame\n",
    "for index, row in geo_df_bad.iterrows():\n",
    "\n",
    "    # Use Google Places API to find all locations for the business\n",
    "    places_result = gmaps.places(query=row['restaurant'])\n",
    "\n",
    "    # Extract place names and addresses\n",
    "    locations = [(place['name'], place['formatted_address']) for place in places_result.get('results', [])]\n",
    "\n",
    "    # Geocode each location\n",
    "    for loc in locations:\n",
    "        place_result = gmaps.geocode(loc[1])\n",
    "        if place_result:\n",
    "            geocode_results_bad.append({\n",
    "                \"restaurant\": row['restaurant'],\n",
    "                \"sandwich_name\": row['sandwich_name'],\n",
    "                \"formatted_address\": loc[1],\n",
    "                \"address\": row['address'],\n",
    "                \"restaurant_api\": loc[0],\n",
    "                \"latitude\": place_result[0]['geometry']['location']['lat'],\n",
    "                \"longitude\": place_result[0]['geometry']['location']['lng'],\n",
    "                \"sandwich_name\": row['sandwich_name'],\n",
    "                \"image_url\": row['image_url'],\n",
    "                \"website\": row['website'],\n",
    "                \"description\": row['description'],\n",
    "                \"price\": row['price']\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocode_results_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_bad_results = pd.DataFrame(geocode_results_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_bad_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_bad_results[['restaurant', 'restaurant_api', 'formatted_address', 'latitude', 'longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_to_remove = [\n",
    "    \"Old Xi’an\", \n",
    "    \"Authentic Xi’an Flavor\", \n",
    "    \"Bites of Xi'an\", \n",
    "    \"Famous Sichuan\", \n",
    "    \"Bites of Xian\", \n",
    "    \"Union Market\", \n",
    "    \"Caputo's Fine Foods\",\n",
    "    \"Union Market\",\n",
    "    \"Willy Deli Grocery Corp\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the bad restuarants\n",
    "geo_df_bad_results_filtered = geo_df_bad_results[\n",
    "    ~geo_df_bad_results['restaurant_api'].str.contains('|'.join(restaurants_to_remove), regex=True)\n",
    "].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_bad_results_filtered['address'] = geo_df_bad_results_filtered['formatted_address'].str.replace(', United States', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_bad_results_filtered[['restaurant', 'restaurant_api', 'address', 'latitude', 'longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_bad_results_filtered.duplicated(subset=['restaurant', 'address']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_bad_results_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_ok.duplicated(subset=['sandwich_name', 'address']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_ok.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the only col i don't want is \"meta_data\"\n",
    "\n",
    "cols_i_want = [\n",
    "    'sandwich_name',\n",
    "    'restaurant',  \n",
    "    'description', \n",
    "    'image_url', \n",
    "    'website', \n",
    "    'price', \n",
    "    'address', \n",
    "    'formatted_address', \n",
    "    'latitude', \n",
    "    'longitude'               \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df_ok.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df_bad_results_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df_bad_results_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df_ok.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_cat_df = pd.concat([geo_df_ok[cols_i_want], geo_df_bad_results_filtered[cols_i_want]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_cat_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_bad_results_filtered.shape[0] + geo_df_ok.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_cat_df.duplicated(subset=['sandwich_name', 'address']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Interactive Google Map\n",
    "\n",
    "https://www.iconicsandwiches.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** \n",
    "\n",
    "Generate an HTML template that displays an interactive Google Map with custom sandwich markers. When a marker is clicked, a Bootstrap modal should appear, showing sandwich details (restaurant name, sandwich name, price, description, image, location, website link, and a \"Get Directions\" link).\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Bootstrap for styling, including a fixed-top navbar and a fixed-bottom footer.\n",
    "- Google Maps API for interactive map functionality.\n",
    "- Custom Markers: Each marker should use a sandwich image as its icon.\n",
    "- Bootstrap Modal: The modal should display:\n",
    "   - Sandwich Name\n",
    "   - Restaurant Name\n",
    "   - Price\n",
    "   - Description\n",
    "   - Sandwich Image\n",
    "   - Restaurant Location\n",
    "   - Clickable Website Link\n",
    "   - \"Get Directions\" link to Google Maps\n",
    "\n",
    "**Data Structure:** \n",
    "\n",
    "The list of sandwich locations should be stored in an array of JavaScript objects. Each object should include:\n",
    "\n",
    "{  \n",
    "   'sandwich_name': 'Chorizo egg sandwich',  \n",
    "   'restaurant': 'C&B',  \n",
    "   'lat': 40.7250765,  \n",
    "   'lng': -73.9816951,  \n",
    "   'img': 'https://static01.nytimes.com/newsgraphics/2024-05-21-sandwich/_images/breakfast2-@@-300.webp',  \n",
    "   'address': '178 East Seventh Street (Avenue B)',  \n",
    "   'price': 13,  \n",
    "   'description': 'Many people wouldn’t guess that C & B, .....',  \n",
    "   'website': 'candbnyc.com'  \n",
    "}\n",
    "\n",
    "**Event Handling:** \n",
    "\n",
    "Clicking a marker should populate the modal with data from the corresponding object and display the modal.\n",
    "\n",
    "**Additional Styling & Functionality:**\n",
    "\n",
    "- The map should take up 100% width and full viewport height (100vh).\n",
    "- The modal should be centered and mobile-friendly (modal-lg).\n",
    "- Use Bootstrap utility classes for alignment and spacing.\n",
    "- Include jQuery and Bootstrap scripts for modal functionality.\n",
    "\n",
    "**Expected Output:** \n",
    "\n",
    "A full HTML, CSS, and JavaScript template that meets all the above specifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print python dictionaries for google map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sandwich_name', 'restaurant', 'description', 'image_url', 'website',\n",
       "       'price', 'address', 'formatted_address', 'latitude', 'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_cat_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = []\n",
    "for index, row in geo_cat_df.iterrows():\n",
    "    location = {\n",
    "        'sandwich_name': row['sandwich_name'],\n",
    "        'restaurant': row['restaurant'],\n",
    "        'lat': row['latitude'],\n",
    "        'lng': row['longitude'],\n",
    "        'img': row['image_url'],\n",
    "        'address': row['address'],\n",
    "        'price': row['price'],\n",
    "        'description': row['description'],\n",
    "        'website': row['website']\n",
    "    }\n",
    "    locations.append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove backgrounds from images\n",
    "\n",
    "This was a pita pit of despair. ChatGPT led my to trying several python packages I could not install. I updated my python install. Still, nothing. That is until I discovered the image pig api on reddit.\n",
    "\n",
    "https://imagepig.com/docs/#cutout\n",
    "\n",
    "<br>\n",
    "\n",
    "*Prompt 0 Chain-of-Thought*\n",
    "\n",
    "I need to use the api below to remove the background from images. The image URLs are stored in a df column.\n",
    "\n",
    "```python\n",
    "from imagepig import ImagePig\n",
    "\n",
    "imagepig = ImagePig(\"your-api-key\")\n",
    "result = imagepig.cutout(\"https://imagepig.com/static/street.jpeg\")\n",
    "result.save(\"cutout.png\")\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "*Prompt 1 Chain-of-Thought*\n",
    "\n",
    "Write the images to an 'output_dir'; check if it exists first, if not create it\n",
    "\n",
    "<br>\n",
    "\n",
    "*Prompt 2 Chain-of-Thought*\n",
    "\n",
    "Name the image after the df['sandwich_name'] column\n",
    "  \n",
    "<br>\n",
    "  \n",
    "*Prompt 3 Chain-of-Thought*\n",
    "\n",
    "There is a rate limit, i can only submit two images per minute\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: chorizo_egg_sandwich.png\n",
      "Processed and saved: smoked_salmon_sandwich.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: becl.png\n",
      "Processed and saved: the_newhouse.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: shake_shack_breakfast_sandwich.png\n",
      "Processed and saved: the_italian.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: the_casa.png\n",
      "Processed and saved: egg_potato_and_cheese_hero.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: the_soho.png\n",
      "Processed and saved: the_bomb.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: hlt.png\n",
      "Processed and saved: crispy_chickn.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: scuttlebutt.png\n",
      "Processed and saved: collard_greens_sandwich_on_focaccia.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: french_onion_sandwich.png\n",
      "Processed and saved: vegan_bbq_pork_bao_bun.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: vegan_lobster_roll.png\n",
      "Processed and saved: pastrami_on_rye.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: pastrami_egg_and_cheese.png\n",
      "Processed and saved: wagyu_pastrami_sando.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: the_no_7.png\n",
      "Processed and saved: classic_bagel_and_lox.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: chopped_cheese.png\n",
      "Processed and saved: the_governors_choice.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: caviar_sandwich.png\n",
      "Processed and saved: roll-n-roast_beef.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: do_it_for_johnny.png\n",
      "Processed and saved: chopped_salad_sandwich.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: prosciutto__butter.png\n",
      "Processed and saved: porchettaboutit.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: dill_party.png\n",
      "Processed and saved: chicken_katsu_club.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: confit_tuna_melt.png\n",
      "Processed and saved: tuna_salad.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: patty_melt.png\n",
      "Processed and saved: the_pho_sandwich.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: shawarma_east_pita.png\n",
      "Processed and saved: bánh_mì_xiú_mai.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: hot_ham_sandwich.png\n",
      "Processed and saved: short_rib_barbacoa_cemita.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: cubano_sandwich.png\n",
      "Processed and saved: focaccia_sandwich.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: chopped_chicharrón_and_brisket_sandwich.png\n",
      "Processed and saved: bulgogi_fatboy.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: mille-feuille_pork_katsu_sando.png\n",
      "Processed and saved: char_siu_bk_rib.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: the_lumberjack.png\n",
      "Processed and saved: the_lumberjack.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: the_lumberjack.png\n",
      "Processed and saved: the_lumberjack.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: the_lumberjack.png\n",
      "Processed and saved: the_lumberjack.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: the_lumberjack.png\n",
      "Processed and saved: the_lumberjack.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: the_lumberjack.png\n",
      "Processed and saved: egg_katsu_sando.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: chicken_parm_hero.png\n",
      "Processed and saved: chicken_parm_hero.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: uncle_rocco.png\n",
      "Processed and saved: uncle_rocco.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: uncle_rocco.png\n",
      "Processed and saved: uncle_rocco.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: michelangelo.png\n",
      "Processed and saved: michelangelo.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: michelangelo.png\n",
      "Processed and saved: michelangelo.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: michelangelo.png\n",
      "Processed and saved: michelangelo.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: michelangelo.png\n",
      "Processed and saved: veg-italian_sub.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: veg-italian_sub.png\n",
      "Processed and saved: veg-italian_sub.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: chicken_caesar_wrap.png\n",
      "Processed and saved: chicken_caesar_wrap.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: spicy_cumin_lamb_burger.png\n",
      "Processed and saved: spicy_cumin_lamb_burger.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: spicy_cumin_lamb_burger.png\n",
      "Processed and saved: spicy_cumin_lamb_burger.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: spicy_cumin_lamb_burger.png\n",
      "Processed and saved: spicy_cumin_lamb_burger.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: spicy_cumin_lamb_burger.png\n",
      "Processed and saved: spicy_cumin_lamb_burger.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: spicy_cumin_lamb_burger.png\n",
      "Processed and saved: spicy_cumin_lamb_burger.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: spicy_cumin_lamb_burger.png\n",
      "Processed and saved: spicy_cumin_lamb_burger.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: spicy_cumin_lamb_burger.png\n",
      "Processed and saved: oxtail_macpatty.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: oxtail_macpatty.png\n",
      "Processed and saved: oxtail_macpatty.png\n",
      "Rate limit reached. Sleeping for 60 seconds...\n",
      "Processed and saved: lil_rowdy_and_big_rowdy.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import re\n",
    "from imagepig import ImagePig\n",
    "\n",
    "# Define the directory for saving processed images\n",
    "# output_dir = \"processed_images_new_run\"\n",
    "\n",
    "# Check if the directory exists and delete all files inside\n",
    "# if os.path.exists(output_dir):\n",
    "#     for file in os.listdir(output_dir):\n",
    "#         file_path = os.path.join(output_dir, file)\n",
    "#         if os.path.isfile(file_path):\n",
    "#             os.remove(file_path)\n",
    "#         elif os.path.isdir(file_path):\n",
    "#             shutil.rmtree(file_path)  # If there are subdirectories, delete them\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get PIG API key\n",
    "pig_api_key = os.getenv(\"PIG_API_KEY\")\n",
    "\n",
    "# Initialize ImagePig API\n",
    "imagepig = ImagePig(pig_api_key)\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"Sanitize sandwich name to create a valid filename.\"\"\"\n",
    "    name = re.sub(r'[^\\w\\s-]', '', name)  # Remove special characters\n",
    "    name = name.replace(\" \", \"_\")  # Replace spaces with underscores\n",
    "    return name.lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Loop through each image URL in the 'image' column and process it\n",
    "image_paths = []  # List to store paths of the processed images\n",
    "for idx, (img_url, sandwich_name) in enumerate(zip(merged_df['image_final'], merged_df['sandwich'])):\n",
    "    try:\n",
    "        # Process the image using ImagePig\n",
    "        result = imagepig.cutout(img_url)\n",
    "        \n",
    "        # Define the output filename using the sanitized sandwich name\n",
    "        sanitized_name = sanitize_filename(sandwich_name)\n",
    "        output_filename = f\"{sanitized_name}.png\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        # Save the result to a file\n",
    "        result.save(output_path)\n",
    "        \n",
    "        print(f\"Processed and saved: {output_filename}\")\n",
    "        \n",
    "        # Store the output path in the list\n",
    "        image_paths.append(output_path)\n",
    "\n",
    "        # If we've processed two images, sleep for a minute to respect the rate limit\n",
    "        if (idx + 1) % 2 == 0:\n",
    "            print(\"Rate limit reached. Sleeping for 60 seconds...\")\n",
    "            time.sleep(60)  # Sleep for 60 seconds (1 minute)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process image at {img_url}: {e}\")\n",
    "        image_paths.append(None)  # In case of failure, append None\n",
    "\n",
    "# Add the image paths back to the DataFrame in the 'images_no_bg' column\n",
    "merged_df['images_no_bg'] = image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = []\n",
    "for index, row in geo_cat_df.iterrows():\n",
    "    location = {\n",
    "        'sandwich_name': row['sandwich_name'],\n",
    "        'restaurant': row['restaurant'],\n",
    "        'lat': row['latitude'],\n",
    "        'lng': row['longitude'],\n",
    "        'img': row['image_url'],\n",
    "        'address': row['address'],\n",
    "        'price': row['price'],\n",
    "        'description': row['description'],\n",
    "        'website': row['website'],\n",
    "        'img_no_bg': row['images_no_bg']\n",
    "    }\n",
    "    locations.append(location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
